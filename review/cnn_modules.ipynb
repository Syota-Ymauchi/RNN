{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "109dba5f-634c-4d89-8055-339d2ad2b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe199c5-2e6c-41c1-8744-1fbc3b3a2fb6",
   "metadata": {},
   "source": [
    "### Swish活性化関数をスクラッチ実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5e423eb9-09bc-4c58-b822-8d9e01e66cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def __init__(self, beta=1.0):      \n",
    "        super().__init__()\n",
    "        \n",
    "        self.beta = beta    \n",
    "        \n",
    "    def forward(self, z):\n",
    "        return z * self.beta* torch.sigmoid(z)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f7a409dc-3bf7-4fad-be47-39fc68800708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1858, -0.1523,  0.8542],\n",
       "          [ 0.2065, -0.1659, -0.2387],\n",
       "          [-0.0512, -0.2001, -0.0118]]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### テスト\n",
    "input_tensor = torch.randn(1, 1, 3, 3)\n",
    "swish = Swish()\n",
    "out = swish(input_tensor)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c77a5a-a6e6-48b7-b49d-c1b68a6863e1",
   "metadata": {},
   "source": [
    "### PytorchのSwish活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ea462803-860f-4984-8965-882cf40ab8bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1858, -0.1523,  0.8542],\n",
       "          [ 0.2065, -0.1659, -0.2387],\n",
       "          [-0.0512, -0.2001, -0.0118]]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.SiLU()(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea203c3-0f5b-4615-8c1e-4579a5374a05",
   "metadata": {},
   "source": [
    "### ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7deb36f1-0244-48bd-80f3-fd0398b5a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, stride=1, activation='relu'):\n",
    "        super().__init__()\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'swish':\n",
    "            self.activation = nn.SiLU()\n",
    "        else:\n",
    "            raise ValueError('not support your activation. Choose from [\"relu\", \"swish\"]')\n",
    "        self.main_conv = nn.Sequential(        \n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            self.activation,\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch)\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if in_ch != out_ch or stride !=1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "           \n",
    "    def forward(self, x):\n",
    "        out = self.main_conv(x) \n",
    "        out += self.shortcut(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4a03115d-61ed-4c50-9c5c-582fe0944f44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 28, 28])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テスト\n",
    "input_tensor = torch.randn(8, 3, 28, 28)\n",
    "residual = ResidualBlock(3, 64, stride=1, activation='swish')\n",
    "out = residual(input_tensor) \n",
    "out.size() # [3, 64, 28, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb3d5c-e6ed-47db-a72a-b3534974390d",
   "metadata": {},
   "source": [
    "### pre-activation residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1fd3349c-0dc5-409c-84ed-15605fbccc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActivationResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch, stride=1, activation='relu'):\n",
    "        super().__init__()\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'swish':\n",
    "            self.activation = nn.SiLU()\n",
    "        else:\n",
    "            raise ValueError('not support your activation. Choose from [\"relu\", \"swish\"]')\n",
    "        self.main_conv = nn.Sequential(        \n",
    "            nn.BatchNorm2d(in_ch),\n",
    "            self.activation,\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            self.activation,\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        )\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if in_ch != out_ch or stride !=1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "           \n",
    "    def forward(self, x):\n",
    "        out = self.main_conv(x) \n",
    "        out += self.shortcut(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "33619990-4deb-4998-a4d4-b3bb39febcb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 64, 28, 28])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テスト\n",
    "input_tensor = torch.randn(8, 3, 28, 28)\n",
    "preresidual = PreActivationResidualBlock(3, 64, stride=1, activation='relu')\n",
    "out = preresidual(input_tensor) \n",
    "out.size() # [3, 64, 28, 28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b390e3-3ac2-4b53-826c-36b42514846d",
   "metadata": {},
   "source": [
    "### Bottleneck構造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d225f2f9-7cd9-469a-99b1-ed24b62cb721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckStracture(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1, activation='relu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'swish':\n",
    "            self.activation = nn.SiLU()\n",
    "        else:\n",
    "            raise ValueError('not support your activation. Choose from [\"relu\", \"swish\"]')\n",
    "    \n",
    "        self.main_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            self.activation,\n",
    "            nn.Conv2d(out_ch, out_ch*4, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch*4),          \n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if in_ch != out_ch*4 or stride !=1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch*4, kernel_size=1, stride=stride, padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_ch*4)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = self.main_conv(x) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c0847c48-7018-4898-8039-2bd6911b3a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 28, 28])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# テスト\n",
    "input_tensor = torch.randn(3, 256, 28, 28)\n",
    "bottleneck = BottleneckStracture(256, 64, stride=1, activation='relu')\n",
    "out = bottleneck(input_tensor)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
